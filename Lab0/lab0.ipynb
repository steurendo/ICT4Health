{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-10-23T10:31:36.150839200Z",
     "start_time": "2023-10-23T10:31:35.005951700Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "<module 'minimization' from 'C:\\\\Users\\\\steur\\\\PycharmProjects\\\\ICT4Health\\\\Lab0\\\\minimization.py'>"
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import importlib\n",
    "import minimization as minim\n",
    "from minimization import SolverLLS, SolverGradient, SolverSteepestDescent\n",
    "\n",
    "importlib.reload(minim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "np.random.seed(72)\n",
    "\n",
    "Np = 10\n",
    "Nf = 6\n",
    "A = np.random.randn(Np, Nf)\n",
    "w = np.random.randn(Nf)  # here we emulate w just to check that the algo works\n",
    "y = A @ w"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-17T13:41:10.638385700Z",
     "start_time": "2023-10-17T13:41:10.601114900Z"
    }
   },
   "id": "fda40cb5391faf1"
  },
  {
   "cell_type": "markdown",
   "source": [
    "LLS algorithm section.\n",
    "If the problem is well-conditioned (all rows and cols are LI), the LLS will give a good result in one shot, because this is a linear system."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fdb5a82b6dfd7f53"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"LLS\" method results\n",
      "Estimated w_hat:  [-1.02881857 -1.19384518  0.49348782 -1.6663294  -0.09966455 -0.80184784]\n",
      "True vector:  [-1.02881857 -1.19384518  0.49348782 -1.6663294  -0.09966455 -0.80184784]\n",
      "Square error ||y - A*w_hat||^2 =  1.41501924874019e-29\n",
      "Time elapsed:  11033.5\n"
     ]
    }
   ],
   "source": [
    "solver = SolverLLS(A, y)\n",
    "t0 = time.time_ns()\n",
    "solver.solve()\n",
    "t1 = time.time_ns()\n",
    "\n",
    "w_hat = solver.result\n",
    "e = y - A @ w_hat\n",
    "\n",
    "print('\"LLS\" method results')\n",
    "print('Estimated w_hat: ', w_hat)\n",
    "print('True vector: ', w)\n",
    "print('Square error ||y - A*w_hat||^2 = ', np.linalg.norm(e) ** 2)\n",
    "print('Time elapsed: ', (t1 - t0) / 1000)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-16T15:04:32.347742900Z",
     "start_time": "2023-10-16T15:04:32.321460800Z"
    }
   },
   "id": "f18b0aaa137ef7fd"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Gradient algorithm section.\n",
    "The gradient algorithm needs a bunch of iterations, but it will give a good result."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "49b5163d90b7dc86"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Gradient algorithm\" method results\n",
      "Estimated w_hat:  [-1.02881857 -1.19384518  0.49348782 -1.6663294  -0.09966455 -0.80184784]\n",
      "True vector:  [-1.02881857 -1.19384518  0.49348782 -1.6663294  -0.09966455 -0.80184784]\n",
      "Square error ||y - A*w_hat||^2 =  9.005340271163614e-29\n",
      "Time elapsed:  29856.7\n"
     ]
    }
   ],
   "source": [
    "solver = SolverGradient(A, y)\n",
    "t0 = time.time_ns()\n",
    "solver.solve(gamma=1e-2, Nit=0)\n",
    "t1 = time.time_ns()\n",
    "\n",
    "w_hat = solver.result\n",
    "e = y - A @ w_hat\n",
    "\n",
    "print('\"Gradient algorithm\" method results')\n",
    "print('Estimated w_hat: ', w_hat)\n",
    "print('True vector: ', w)\n",
    "print('Square error ||y - A*w_hat||^2 = ', np.linalg.norm(e) ** 2)\n",
    "print('Time elapsed: ', (t1 - t0) / 1000)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-16T15:04:46.440884200Z",
     "start_time": "2023-10-16T15:04:46.387032600Z"
    }
   },
   "id": "a44a7b2fa35d929e"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Steepest Descent algorithm section.\n",
    "This algorithm is quite efficient in any case, especially compared to the Gradient algorithm."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "59597015b256afbf"
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "culo\n",
      "\"Steepest descent\" algorithm method results\n",
      "Estimated w_hat:  [-1.02881857 -1.19384518  0.49348782 -1.6663294  -0.09966455 -0.80184784]\n",
      "True vector:  [-1.02881857 -1.19384518  0.49348782 -1.6663294  -0.09966455 -0.80184784]\n",
      "Square error ||y - A*w_hat||^2 =  8.38164711797325e-31\n",
      "Time elapsed:  18059.3\n"
     ]
    }
   ],
   "source": [
    "solver = SolverSteepestDescent(A, y)\n",
    "t0 = time.time_ns()\n",
    "solver.solve(Nit=0)\n",
    "t1 = time.time_ns()\n",
    "\n",
    "w_hat = solver.result\n",
    "e = y - A @ w_hat\n",
    "\n",
    "print('\"Steepest descent\" algorithm method results')\n",
    "print('Estimated w_hat: ', w_hat)\n",
    "print('True vector: ', w)\n",
    "print('Square error ||y - A*w_hat||^2 = ', np.linalg.norm(e) ** 2)\n",
    "print('Time elapsed: ', (t1 - t0) / 1000)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-17T13:50:54.881928900Z",
     "start_time": "2023-10-17T13:50:54.684415900Z"
    }
   },
   "id": "42bfb69ba00c1f83"
  },
  {
   "cell_type": "markdown",
   "source": [
    "The Gradient algorithm, takes more time than the LLS and is less efficient than the Steepest Descent algorithm.\n",
    "Using a big learning coefficient the algorithm won't give good results, while it gives its best with a value of 1e-2.\n",
    "Using 1e-2 as learning coefficient, if the number of iterations stays around 1000 doesn't reach the\n",
    "same result's quality as for the other two methods.\n",
    "Using the relative error as a stopping condition, it takes more time, but it reaches a good result.\n",
    "Questions:\n",
    "1 - LLS works well with linear (and well-conditioned) problems, while the Gradient algorithm is less efficient but\n",
    "    adaptable also for non-linear problems, so the expected result is the same (depending on the learning coefficient).\n",
    "2 - The Steepest Descent algorithm takes fewer steps than the Gradient algorithm, but the learning coefficient has to\n",
    "    be computed for each iteration. As for the Gradient algorithm, the result should be the same of LLS's.\n",
    "3 - Numerical results nearly impossible, since being Gradient and Steepest Descent algorithms iterative they stops after\n",
    "    a condition is met: the result changes through floating numbers at each iteration, and it's pratically impossible\n",
    "    to have the same result as for a \"one shot\" method. "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "38e4c5da784e8e51"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "a88bb936ce6792a6"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
