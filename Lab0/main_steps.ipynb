{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "from minimization import SolverLLS, SolverGradient, SolverSteepestDescent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "np.random.seed(72)\n",
    "\n",
    "Np = 10\n",
    "Nf = 6\n",
    "A = np.random.randn(Np, Nf)\n",
    "w = np.random.randn(Nf)  # here we emulate w just to check that the algo works\n",
    "y = A @ w"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fda40cb5391faf1"
  },
  {
   "cell_type": "markdown",
   "source": [
    "LLS algorithm section.\n",
    "It will give a good example in one shot, (probably) because this is a linear system."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fdb5a82b6dfd7f53"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "solver = SolverLLS(A, y)\n",
    "t0 = time.time_ns()\n",
    "solver.solve()\n",
    "t1 = time.time_ns()\n",
    "\n",
    "w_hat = solver.result\n",
    "e = y - A @ w_hat\n",
    "\n",
    "print('\"LLS\" method results')\n",
    "print('Estimated w_hat: ', w_hat)\n",
    "print('True vector: ', w)\n",
    "print('Square error ||y - A*w_hat||^2 = ', np.linalg.norm(e) ** 2)\n",
    "print('Time elapsed: ', (t1 - t0) / 1000)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f18b0aaa137ef7fd"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Gradient algorithm section.\n",
    "The gradient algorithm needs a bunch of iterations, but it will give a good result."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "49b5163d90b7dc86"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "solver = SolverGradient(A, y)\n",
    "t0 = time.time_ns()\n",
    "solver.solve(gamma=1e-2, Nit=0)\n",
    "t1 = time.time_ns()\n",
    "\n",
    "w_hat = solver.result\n",
    "e = y - A @ w_hat\n",
    "\n",
    "print('\"Gradient algorithm\" method results')\n",
    "print('Estimated w_hat: ', w_hat)\n",
    "print('True vector: ', w)\n",
    "print('Square error ||y - A*w_hat||^2 = ', np.linalg.norm(e) ** 2)\n",
    "print('Time elapsed: ', (t1 - t0) / 1000)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a44a7b2fa35d929e"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Steepest Descent algorithm section.\n",
    "This algorithm is quite efficient in any case, especially compared to the Gradient algorithm."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "59597015b256afbf"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "solver = SolverSteepestDescent(A, y)\n",
    "t0 = time.time_ns()\n",
    "solver.solve(Nit=0)\n",
    "t1 = time.time_ns()\n",
    "\n",
    "w_hat = solver.result\n",
    "e = y - A @ w_hat\n",
    "\n",
    "print('\"Steepest descent\" algorithm method results')\n",
    "print('Estimated w_hat: ', w_hat)\n",
    "print('True vector: ', w)\n",
    "print('Square error ||y - A*w_hat||^2 = ', np.linalg.norm(e) ** 2)\n",
    "print('Time elapsed: ', (t1 - t0) / 1000)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "42bfb69ba00c1f83"
  },
  {
   "cell_type": "markdown",
   "source": [
    "The Gradient algorithm, takes more time than the LLS and is less efficient than the Steepest Descent algorithm.\n",
    "Using a big learning coefficient the algorithm won't give good results, while it gives its best with a value of 1e-2.\n",
    "Using 1e-2 as learning coefficient, if the number of iterations stays around 1000 doesn't reach the\n",
    "same result's quality as for the other two methods.\n",
    "Using the relative error as a stopping condition, it takes more time, but it reaches a good result.\n",
    "Questions:\n",
    "1 - LLS works well with linear (and well-conditioned) problems, while the Gradient algorithm is less efficient but\n",
    "    adaptable also for non-linear problems, so the expected result is the same (depending on the learning coefficient).\n",
    "2 - The Steepest Descent algorithm takes fewer steps than the Gradient algorithm, but the learning coefficient has to\n",
    "    be computed for each iteration. As for the Gradient algorithm, the result should be the same of LLS's.\n",
    "3 - Numerical results nearly impossible, since being Gradient and Steepest Descent algorithms iterative they stops after\n",
    "    a condition is met: the result changes through floating numbers at each iteration, and it's pratically impossible\n",
    "    to have the same result as for a \"one shot\" method.  "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "38e4c5da784e8e51"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
